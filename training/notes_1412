from library_imports import *
from main import *
from preprocessing import *
import scipy.io
import numpy as np
import pandas as pd
import csv
from csv import reader
import itertools
from numpy import genfromtxt


def adj_matrix_builder(elements, num_nodes):
    '''

    :param elements: a .csv file which contains the elements of the FE model.
    :param num_nodes: number of nodes.
    :return: the adjacency matrix.
    '''

    A = np.zeros((num_nodes, num_nodes))

    with open(elements, 'r') as read_obj:   # pass the file object to reader() to get the reader object
        csv_reader = reader(read_obj)       # Iterate over each row in the csv using reader object
        for row in csv_reader:              # row variable is a list that represents a row in csv

            node_1 = int(row[0]) - 1
            node_2 = int(row[1]) - 1
            node_3 = int(row[2]) - 1
            node_4 = int(row[3]) - 1

            A[node_1, node_2] = 1
            A[node_2, node_1] = 1

            A[node_1, node_3] = 1
            A[node_3, node_1] = 1

            A[node_1, node_4] = 1
            A[node_4, node_1] = 1

            A[node_2, node_3] = 1
            A[node_3, node_2] = 1

            A[node_2, node_4] = 1
            A[node_4, node_2] = 1

            A[node_3, node_4] = 1
            A[node_4, node_3] = 1

    return A

# --------------------------------------------------------------------------------------------------

def adj_matrix_format(A, filename):
    '''

    :param A: an adjacency matrix
    :param filename: the name of the file where the formatted adjacency matrix will be stored.
    :return: -
    '''

    num_rows = np.size(A,0)
    num_cols = np.size(A,1)

    with open(filename, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile, delimiter=',')

        for i in range(num_rows):

            for j in range(num_cols):
                list = []
                if A[i,j]!=0:
                    list.append(j+1)
                    list.append(i+1)
                    writer.writerow(list)

# --------------------------------------------------------------------------------------------------

def adj_matrix_full_format(A, num_nodes, num_dir, t_steps, filename):
    '''

    :param A: an adjacency matrix
    :param filename: the name of the file where the formatted adjacency matrix will be stored.
    :return: -
    '''

    num_rows = np.size(A,0)
    num_cols = np.size(A,1)
    counter = 0

    with open(filename, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile, delimiter=',')

        for k in range(num_dir*t_steps):

            for i in range(num_rows):

                for j in range(num_cols):
                    list = []
                    if A[i,j]!=0:
                        list.append(j+1 + (k * num_nodes))
                        list.append(i+1 + (k * num_nodes))
                        writer.writerow(list)
                        counter = counter + 1
                        print(counter)

# --------------------------------------------------------------------------------------------------

def xyz(filename_xyz):
    '''

    :param filename_xyz: a .csv file containing the x y and z coordinates of the nodes within the FE model
    :return: a panda data frame holding the x y z coordinate information.
    '''

    data = pd.read_csv(filename_xyz, header=None)
    df = pd.DataFrame(data)
    df.columns = ['x', 'y', 'z']

    return df

# --------------------------------------------------------------------------------------------------

def node_material_assign(elements, elements_ID, face_boundary, boundary_marker, num_nodes, filename):
    '''

    :param elements: csv file which contains the elements of the FE model.
    :param elements_ID: a .csv file which assigns every element to a material.
    :param num_nodes: number of nodes within the FE model.
    :param filename: a filename where every NODE will be assigned a material property (as opposed to element).
    :return: a panda data frame holding node material information.
    '''

    node_ID = np.zeros((num_nodes, 1), dtype=int)

    with open(elements, 'r') as read_obj_1:
        with open(elements_ID, 'r') as read_obj_2:

            csv_reader_1 = reader(read_obj_1) # elements
            csv_reader_2 = reader(read_obj_2) # element id

            for row_1, row_2 in itertools.zip_longest(csv_reader_1, csv_reader_2):

                val = int(row_2[0])

                for n in row_1:
                    n = int(n)-1
                    node_ID[n, 0] = val

    with open(face_boundary, 'r') as read_obj_3:
        with open(boundary_marker, 'r') as read_obj_4:

            csv_reader_3 = reader(read_obj_3)  # face boundaries
            csv_reader_4 = reader(read_obj_4)  # boundary id

            for row_3, row_4 in itertools.zip_longest(csv_reader_3, csv_reader_4):

                val_b = int(row_4[0])
                print(val_b)

                for n in row_3:
                    n = int(n) - 1
                    node_ID[n, 0] = val_b

    np.savetxt(filename, node_ID, fmt='%i')

    df = pd.DataFrame(list(node_ID), columns=['Mat_ID'])
    print(df)
    return df

# --------------------------------------------------------------------------------------------------

def node_support_assign(support_nodes, num_nodes, filename):
    '''

    :param support_nodes: a .csv file which indicates the node ID of rigid nodes in the FE model.
    :param num_nodes: number of nodes within the FE model.
    :param filename: the place to store a column of size( num_nodes x 1) where rigid nodes have a value of 1.
    :return: a panda data frame marking rigid nodes.
    '''

    support_list = np.zeros((num_nodes, 1), dtype=np.int)

    with open(support_nodes, 'r') as read_obj:

        csv_reader = reader(read_obj)
        for row in csv_reader:
            n = int(row[0])-1
            support_list[n,0]=1

    np.savetxt(filename, support_list, fmt='%i')
    df = pd.DataFrame(list(support_list), columns=['Rigid_ID'])
    return df

# --------------------------------------------------------------------------------------------------

"""
The format of the feature matrix is:

{Node ID} {x} {y} {z} {material_ID} {Rigid} {Force magnitude} {F_x} {F_y} {F_z}
"""
#
def feature_constructor(num_nodes, df_xyz, df_mat_id, df_rigid_id, filename_pres_nodes, filename_force_dir, magnitude, t_steps):
    '''

    :param num_nodes: Number of nodes within the FE mesh
    :param df_xyz: x y z data frame
    :param df_mat_id:  material id data frame
    :param df_rigid_id: support nodes data frame
    :param filename_pres_nodes:  prescribed load nodes csv file
    :param filename_force_dir:  direction of forces (normal vectors) csv file
    :param magnitude: magnitude of the force
    :param t_steps: number of time steps in the FEA
    :return: features data frame
    '''

    pres_nodes = genfromtxt(filename_pres_nodes, delimiter=',', dtype=int)
    force_dirs = genfromtxt(filename_force_dir, delimiter=',')

    idx_repeat = int(len(force_dirs) / len(pres_nodes))
    pres_nodes_full = []

    for p_node in pres_nodes:
        for j in range(idx_repeat):
            pres_nodes_full.append(int(p_node))

    # print(pres_nodes_full)

    df_f_1 = pd.concat([df_xyz, df_mat_id, df_rigid_id], axis=1)
    df_f_copy = pd.concat([df_xyz, df_mat_id, df_rigid_id], axis=1)

    for i in range(len(pres_nodes_full) * t_steps - 1):
        frame = [df_f_1,df_f_copy]
        df_f_1 = pd.concat(frame)
        df_f_1.reset_index(drop=True, inplace=True)

    step_mag = magnitude/t_steps

    force = []
    pres_nodes_idx = []
    m = 0

    for i in range(len(pres_nodes_full)):

        p_node_force_direction = force_dirs[i, :]

        for j in range(t_steps):

            updated_mag = step_mag * (j+1)

            force.append(updated_mag)
            force.append(p_node_force_direction[0])
            force.append(p_node_force_direction[1])
            force.append(p_node_force_direction[2])

            # ----------------------------------------

            idx = (pres_nodes_full[i] - 1) + (m * num_nodes)
            pres_nodes_idx.append(idx)
            m = m + 1

    force = np.reshape(force, (-1, 4)) # columns of this matrix are: magnitude, F_x, F_y, F_z

    # print(force)


    force_length = t_steps * len(pres_nodes_full) * num_nodes


    M = np.zeros((force_length, 4))

    # print(pres_nodes_idx)

    for k in range(len(pres_nodes_idx)):

        # print('------------')
        # print(pres_nodes_idx[k])
        # print(force[k, 0])
        # print(force[k, 1])
        # print(force[k, 2])
        # print(force[k, 3])
        # print('------------')

        M[pres_nodes_idx[k], 0] = force[k, 0]
        M[pres_nodes_idx[k], 1] = force[k, 1]
        M[pres_nodes_idx[k], 2] = force[k, 2]
        M[pres_nodes_idx[k], 3] = force[k, 3]

    # print('Printing M')
    # print(M)

    df_f_2 = pd.DataFrame(data=M, columns=["Magnitude", "F_x", "F_y", "F_z"])

    # print(df_f_1)
    # print(df_f_2)


    frame_2 = [df_f_1, df_f_2]

    df_features = pd.concat(frame_2, axis=1)

    # print(df_features)

    return df_features

# --------------------------------------------------------------------------------------------------

def feature_constructor_2(num_nodes, df_xyz, df_mat_id, df_rigid_id, filename_pres_nodes,
                          filename_force_dir_x, filename_force_dir_y, filename_force_dir_z, magnitude, t_steps):
    '''
    This function is used for when we want to construct the feature matrix when we are applying prescribe forces to
    multiple nodes at the same time.

    :param num_nodes: Number of nodes within the FE mesh
    :param df_xyz: x y z data frame
    :param df_mat_id:  material id data frame
    :param df_rigid_id: support nodes data frame
    :param filename_pres_nodes:  prescribed load nodes csv file
    :param filename_force_dir:  direction of forces (normal vectors) csv file
    :param magnitude: magnitude of the force
    :param t_steps: number of time steps in the FEA
    :return: features data frame
    '''

    pres_nodes = genfromtxt(filename_pres_nodes, delimiter=',', dtype=int)
    force_dir_x = genfromtxt(filename_force_dir_x, delimiter=',')
    force_dir_y = genfromtxt(filename_force_dir_y, delimiter=',')
    force_dir_z = genfromtxt(filename_force_dir_z, delimiter=',')

    idx_repeat = force_dir_x.shape[1]
    print(idx_repeat)

    df_f_1 = pd.concat([df_xyz, df_mat_id, df_rigid_id], axis=1)
    df_f_copy = pd.concat([df_xyz, df_mat_id, df_rigid_id], axis=1)

    for i in range(idx_repeat * t_steps - 1):
        frame = [df_f_1,df_f_copy]
        df_f_1 = pd.concat(frame)
        df_f_1.reset_index(drop=True, inplace=True)

    step_mag = magnitude/(t_steps * len(pres_nodes))

    force_dir_x_full = []
    force_dir_y_full = []
    force_dir_z_full = []
    force_magnitude = []
    pres_nodes_idx = []
    m = 0


    for i in range(idx_repeat):

        for j in range(t_steps):

            for k in range(len(pres_nodes)):

                force_dir_x_full.append(force_dir_x[k, i])
                force_dir_y_full.append(force_dir_y[k, i])
                force_dir_z_full.append(force_dir_z[k, i])

                updated_mag = step_mag * (j + 1)
                force_magnitude.append(updated_mag)

                idx = int(pres_nodes[k]) - 1 + (num_nodes*m)
                pres_nodes_idx.append(idx)

            m = m + 1

    force_length = t_steps * idx_repeat * num_nodes

    M = np.zeros((force_length, 4))

    # print(force_dir_x_full)
    # print(force_dir_y_full)
    # print(force_dir_z_full)
    # print(pres_nodes_idx)

    for k in range(len(pres_nodes_idx)):

        # print('------------')
        # print(pres_nodes_idx[k])
        # print(force[k, 0])
        # print(force[k, 1])
        # print(force[k, 2])
        # print(force[k, 3])
        # print('------------')

        M[pres_nodes_idx[k], 0] = force_magnitude[k]
        M[pres_nodes_idx[k], 1] = force_dir_x_full[k]
        M[pres_nodes_idx[k], 2] = force_dir_y_full[k]
        M[pres_nodes_idx[k], 3] = force_dir_z_full[k]

    # print('Printing M')
    # print(M)

    df_f_2 = pd.DataFrame(data=M, columns=["Magnitude", "F_x", "F_y", "F_z"])

    # print(df_f_1)
    # print(df_f_2)


    frame_2 = [df_f_1, df_f_2]

    df_features = pd.concat(frame_2, axis=1)

    print(df_features)

    return df_features

# --------------------------------------------------------------------------------------------------

def feature_constructor3(num_nodes, df_xyz, df_mat_id, df_rigid_id, filename_pres_nodes, node_idx, filename_force_dir, magnitude, t_steps):
    '''

    :param num_nodes: Number of nodes within the FE mesh
    :param df_xyz: x y z data frame
    :param df_mat_id:  material id data frame
    :param df_rigid_id: support nodes data frame
    :param filename_pres_nodes:  prescribed load nodes csv file
    :param filename_force_dir:  direction of forces (normal vectors) csv file
    :param magnitude: magnitude of the force
    :param t_steps: number of time steps in the FEA
    :return: features data frame
    '''

    pres_nodes = genfromtxt(filename_pres_nodes, delimiter=',', dtype=int)
    print(pres_nodes)
    p_node = pres_nodes[node_idx-1]
    print(p_node)
    force_dirs = genfromtxt(filename_force_dir, delimiter=',')

    idx_repeat = len(force_dirs)
    pres_nodes_full = []


    for j in range(idx_repeat):
        pres_nodes_full.append(int(p_node))

    print(pres_nodes_full)

    df_f_1 = pd.concat([df_xyz, df_mat_id, df_rigid_id], axis=1)
    df_f_copy = pd.concat([df_xyz, df_mat_id, df_rigid_id], axis=1)

    for i in range(len(pres_nodes_full) * t_steps - 1):
        frame = [df_f_1,df_f_copy]
        df_f_1 = pd.concat(frame)
        df_f_1.reset_index(drop=True, inplace=True)

    step_mag = magnitude/t_steps

    force = []
    pres_nodes_idx = []
    m = 0

    for i in range(len(pres_nodes_full)):

        p_node_force_direction = force_dirs[i, :]

        for j in range(t_steps):

            updated_mag = step_mag * (j+1)

            force.append(updated_mag)
            force.append(p_node_force_direction[0])
            force.append(p_node_force_direction[1])
            force.append(p_node_force_direction[2])

            # ----------------------------------------

            idx = (pres_nodes_full[i] - 1) + (m * num_nodes)
            pres_nodes_idx.append(idx)
            m = m + 1

    force = np.reshape(force, (-1, 4)) # columns of this matrix are: magnitude, F_x, F_y, F_z

    # print(pres_nodes_idx)
    # print(force)


    force_length = t_steps * len(pres_nodes_full) * num_nodes


    M = np.zeros((force_length, 4))

    # print(pres_nodes_idx)

    for k in range(len(pres_nodes_idx)):

        # print('------------')
        # print(pres_nodes_idx[k])
        # print(force[k, 0])
        # print(force[k, 1])
        # print(force[k, 2])
        # print(force[k, 3])
        # print('------------')

        M[pres_nodes_idx[k], 0] = force[k, 0]
        M[pres_nodes_idx[k], 1] = force[k, 1]
        M[pres_nodes_idx[k], 2] = force[k, 2]
        M[pres_nodes_idx[k], 3] = force[k, 3]

    # print('Printing M')
    # print(M)

    df_f_2 = pd.DataFrame(data=M, columns=["Magnitude", "F_x", "F_y", "F_z"])

    # print(df_f_1)
    # print(df_f_2)


    frame_2 = [df_f_1, df_f_2]

    df_features = pd.concat(frame_2, axis=1)

    # print(df_features)

    return df_features

# --------------------------------------------------------------------------------------------------


def graph_indicator(num_nodes, num_p_nodes, num_dirs, t_steps):

    num_graphs = num_p_nodes * num_dirs * t_steps

    graph_labels = []

    for i in range(num_graphs):
        for j in range(num_nodes):
            graph_labels.append(i+1)

    df = pd.DataFrame(graph_labels)

    return df

# --------------------------------------------------------------------------------------------------

def graph_indicator_2(num_nodes, num_dirs, t_steps):

    num_graphs =  num_dirs * t_steps

    graph_labels = []

    for i in range(num_graphs):
        for j in range(num_nodes):
            graph_labels.append(i+1)

    df = pd.DataFrame(graph_labels)

    return df

# --------------------------------------------------------------------------------------------------


def graph_label(num_p_nodes, num_dirs, t_steps):

    num_graphs = num_p_nodes * num_dirs * t_steps

    graph_labels = []

    for i in range(num_graphs):
        graph_labels.append(i+1)

    df = pd.DataFrame(graph_labels)

    return df

# --------------------------------------------------------------------------------------------------

def graph_label_2(num_dirs, t_steps):

    num_graphs =  num_dirs * t_steps

    graph_labels = []

    for i in range(num_graphs):
        graph_labels.append(i+1)

    df = pd.DataFrame(graph_labels)

    return df

# --------------------------------------------------------------------------------------------------


def output_format(filename_output, t_steps): # this part needs to be hardcoded. This code is for when we have 5 time steps
    '''
    This function requires to be hardcoded.
    :param filename_output: an output file holding displacement values at x y z for t_steps
    :param t_steps: number of time steps in the FEA
    :return: a panda data frame
    '''


    output_v1 = genfromtxt(filename_output, delimiter=',')

    start = []
    end = []

    for i in range(t_steps):

        x = 3 * (i + 1)
        start.append(x)
        end.append(x + 3)

    out_1 = output_v1[:, start[0]: end[0]]
    out_2 = output_v1[:, start[1]: end[1]]
    out_3 = output_v1[:, start[2]: end[2]]
    out_4 = output_v1[:, start[3]: end[3]]
    out_5 = output_v1[:, start[4]: end[4]]
    out_6 = output_v1[:, start[5]: end[5]]
    out_7 = output_v1[:, start[6]: end[6]]
    out_8 = output_v1[:, start[7]: end[7]]
    out_9 = output_v1[:, start[8]: end[8]]
    out_10 = output_v1[:, start[9]: end[9]]
    out_11 = output_v1[:, start[10]: end[10]]
    out_12 = output_v1[:, start[11]: end[11]]
    out_13 = output_v1[:, start[12]: end[12]]
    out_14 = output_v1[:, start[13]: end[13]]
    out_15 = output_v1[:, start[14]: end[14]]
    out_16 = output_v1[:, start[15]: end[15]]
    out_17 = output_v1[:, start[16]: end[16]]
    out_18 = output_v1[:, start[17]: end[17]]
    out_19 = output_v1[:, start[18]: end[18]]
    out_20 = output_v1[:, start[19]: end[19]]
    out_21 = output_v1[:, start[20]: end[20]]
    out_22 = output_v1[:, start[21]: end[21]]
    out_23 = output_v1[:, start[22]: end[22]]
    out_24 = output_v1[:, start[23]: end[23]]
    out_25 = output_v1[:, start[24]: end[24]]
    out_26 = output_v1[:, start[25]: end[25]]
    out_27 = output_v1[:, start[26]: end[26]]
    out_28 = output_v1[:, start[27]: end[27]]
    out_29 = output_v1[:, start[28]: end[28]]
    out_30 = output_v1[:, start[29]: end[29]]

    df_1 = pd.DataFrame(data=out_1)
    df_2 = pd.DataFrame(data=out_2)
    df_3 = pd.DataFrame(data=out_3)
    df_4 = pd.DataFrame(data=out_4)
    df_5 = pd.DataFrame(data=out_5)
    df_6 = pd.DataFrame(data=out_6)
    df_7 = pd.DataFrame(data=out_7)
    df_8 = pd.DataFrame(data=out_8)
    df_9 = pd.DataFrame(data=out_9)
    df_10 = pd.DataFrame(data=out_10)
    df_11 = pd.DataFrame(data=out_11)
    df_12 = pd.DataFrame(data=out_12)
    df_13 = pd.DataFrame(data=out_13)
    df_14 = pd.DataFrame(data=out_14)
    df_15 = pd.DataFrame(data=out_15)
    df_16 = pd.DataFrame(data=out_16)
    df_17 = pd.DataFrame(data=out_17)
    df_18 = pd.DataFrame(data=out_18)
    df_19 = pd.DataFrame(data=out_19)
    df_20 = pd.DataFrame(data=out_20)
    df_21 = pd.DataFrame(data=out_21)
    df_22 = pd.DataFrame(data=out_22)
    df_23 = pd.DataFrame(data=out_23)
    df_24 = pd.DataFrame(data=out_24)
    df_25 = pd.DataFrame(data=out_25)
    df_26 = pd.DataFrame(data=out_26)
    df_27 = pd.DataFrame(data=out_27)
    df_28 = pd.DataFrame(data=out_28)
    df_29 = pd.DataFrame(data=out_29)
    df_30 = pd.DataFrame(data=out_30)

    frame = [df_1, df_2, df_3, df_4,
             df_5, df_6, df_7, df_8,
             df_9, df_10, df_11, df_12,
             df_13, df_14, df_15, df_16,
             df_17, df_18, df_19, df_20,
             df_21, df_22, df_23, df_24,
             df_25, df_26, df_27, df_28,
             df_29, df_30]

    df_output = pd.concat(frame)

    df_output.reset_index(drop=True, inplace=True)

    return df_output


# Data set 9: Applying forces to 100 nodes at a time. Forces have 30 different magnitudes and 15 different directions.

def dataset_2(idx):

    input_path = 'Data_Generator/code/input_2/'
    output_path = 'Data_Generator/code/output_2/csv/'

    # Initialization
    elements_filename = input_path + 'elements.csv'
    element_id_filename = input_path + 'element_ID.csv'
    xyz_filename = input_path + 'xyz.csv'
    support_nodes_filename = input_path + 'bcSupportList.csv'


    pres_nodes_filename = input_path + 'bcPrescribeList.csv'
    boundary_faces_filename = input_path + 'boundary_faces.csv'
    boundary_id_filename = input_path + 'boundary_marker.csv'

    num_nodes = 9118  # number of nodes per each graph
    num_p_nodes = 100
    num_dirs = 15
    t_steps = 30
    f_magnitude = 20  # units: Newtons

    if idx == 1:
        formatted_data_path = 'dataset_2/a/'
        filename_force_dir_x = input_path + 'force_dir_x_a.csv'
        filename_force_dir_y = input_path + 'force_dir_y_a.csv'
        filename_force_dir_z = input_path + 'force_dir_z_a.csv'
        offset_output = 0
    elif idx == 2:
        formatted_data_path = 'dataset_2/b/'
        filename_force_dir_x = input_path + 'force_dir_x_b.csv'
        filename_force_dir_y = input_path + 'force_dir_y_b.csv'
        filename_force_dir_z = input_path + 'force_dir_z_b.csv'
        offset_output = 15
    elif idx == 3:
        formatted_data_path = 'dataset_2/c/'
        filename_force_dir_x = input_path + 'force_dir_x_c.csv'
        filename_force_dir_y = input_path + 'force_dir_y_c.csv'
        filename_force_dir_z = input_path + 'force_dir_z_c.csv'
        offset_output = 30
    elif idx == 4:
        formatted_data_path = 'dataset_2/d/'
        filename_force_dir_x = input_path + 'force_dir_x_d.csv'
        filename_force_dir_y = input_path + 'force_dir_y_d.csv'
        filename_force_dir_z = input_path + 'force_dir_z_d.csv'
        offset_output = 45
    elif idx == 5:
        formatted_data_path = 'dataset_2/e/'
        filename_force_dir_x = input_path + 'force_dir_x_e.csv'
        filename_force_dir_y = input_path + 'force_dir_y_e.csv'
        filename_force_dir_z = input_path + 'force_dir_z_e.csv'
        offset_output = 60
    elif idx == 6:
        formatted_data_path = 'dataset_2/f/'
        filename_force_dir_x = input_path + 'force_dir_x_f.csv'
        filename_force_dir_y = input_path + 'force_dir_y_f.csv'
        filename_force_dir_z = input_path + 'force_dir_z_f.csv'
        offset_output = 75
    elif idx == 7:
        formatted_data_path = 'dataset_2/g/'
        filename_force_dir_x = input_path + 'force_dir_x_g.csv'
        filename_force_dir_y = input_path + 'force_dir_y_g.csv'
        filename_force_dir_z = input_path + 'force_dir_z_g.csv'
        offset_output = 90
    elif idx == 8:
        formatted_data_path = 'dataset_2/h/'
        filename_force_dir_x = input_path + 'force_dir_x_h.csv'
        filename_force_dir_y = input_path + 'force_dir_y_h.csv'
        filename_force_dir_z = input_path + 'force_dir_z_h.csv'
        offset_output = 105
    elif idx == 9:
        formatted_data_path = 'dataset_2/i/'
        filename_force_dir_x = input_path + 'force_dir_x_i.csv'
        filename_force_dir_y = input_path + 'force_dir_y_i.csv'
        filename_force_dir_z = input_path + 'force_dir_z_i.csv'
        offset_output = 120
    elif idx == 10:
        formatted_data_path = 'dataset_2/j/'
        filename_force_dir_x = input_path + 'force_dir_x_j.csv'
        filename_force_dir_y = input_path + 'force_dir_y_j.csv'
        filename_force_dir_z = input_path + 'force_dir_z_j.csv'
        offset_output = 135
    elif idx == 11:
        formatted_data_path = 'dataset_2/h/'
        filename_force_dir_x = input_path + 'force_dir_x_k.csv'
        filename_force_dir_y = input_path + 'force_dir_y_k.csv'
        filename_force_dir_z = input_path + 'force_dir_z_k.csv'
        offset_output = 150
    else:
        raise ValueError('Dataset for offsets above 11 do not exist.')

    ######################################################################################################
    # Output file names: 1) formatted input files, 2) intermediate files, 3) formatted output files
    # ** We only need to upload the formatted input and output files to Google Drive

    # 1) formatted input files
    adj_matrix_filename = formatted_data_path + 'A.csv'
    node_att_filename = formatted_data_path + 'node_attributes.csv'
    graph_indicator_filename = formatted_data_path + 'graph_indicator.csv'
    graph_labels_filename = formatted_data_path + 'graph_labels.csv'
    # -----------------------------------------------------------------------------
    # 2) intermediate files
    node_material_id_filename = formatted_data_path + 'node_material_id.csv'
    rigid_nodes_filename = formatted_data_path + 'rigid_nodes_id.csv'
    adj_matrix_short_filename = formatted_data_path + 'A_partial.csv'
    # -----------------------------------------------------------------------------
    # 3) formatted output files
    output_displacement_filename = formatted_data_path + 'output_displacement.csv'

    ###################################################################################################################

    # Creating the adjacency matrix

    # A = adj_matrix_builder(elements_filename, num_nodes)
    # adj_matrix_full_format(A, num_nodes, num_dirs, t_steps, adj_matrix_filename)

    ###################################################################################################################

    # Creating the feature matrix

    df_xyz = xyz(xyz_filename)
    df_mat_id = node_material_assign(elements_filename, element_id_filename, boundary_faces_filename, boundary_id_filename,
                                     num_nodes, node_material_id_filename)
    df_rigid_id = node_support_assign(support_nodes_filename, num_nodes, rigid_nodes_filename)

    df_features = feature_constructor_2(num_nodes, df_xyz, df_mat_id, df_rigid_id, pres_nodes_filename,
                              filename_force_dir_x, filename_force_dir_y, filename_force_dir_z, f_magnitude, t_steps)

    df_features.to_csv(node_att_filename, encoding='utf-8', header=False, index=False)

    ###################################################################################################################

    # Creating the Graph indicator file (which determines which node belongs to which graph)

    node_graph_labels = graph_indicator_2(num_nodes, num_dirs, t_steps)
    node_graph_labels.to_csv(graph_indicator_filename, header=False, index=False)

    # ###################################################################################################################
    #
    # # Graph labels

    graph_labels = graph_label_2(num_dirs, t_steps)
    graph_labels.to_csv(graph_labels_filename, header=False, index=False)

    # ###################################################################################################################
    #
    # Output for 100 load nodes, 15 directions, and 30 time steps. Force is applied to all nodes at once.

    output_files = []

    for i in range(1, num_dirs + 1):
        s1 = "displacement_nodes_dir_"
        s2 = str(i + offset_output)
        s3 = ".csv"
        s = output_path + s1 + s2 + s3
        print(s)
        output_files.append(s)

    df_1 = output_format(output_files[0], t_steps)
    df_2 = output_format(output_files[1], t_steps)
    df_3 = output_format(output_files[2], t_steps)
    df_4 = output_format(output_files[3], t_steps)
    df_5 = output_format(output_files[4], t_steps)
    df_6 = output_format(output_files[5], t_steps)
    df_7 = output_format(output_files[6], t_steps)
    df_8 = output_format(output_files[7], t_steps)
    df_9 = output_format(output_files[8], t_steps)
    df_10 = output_format(output_files[9], t_steps)
    df_11 = output_format(output_files[10], t_steps)
    df_12 = output_format(output_files[11], t_steps)
    df_13 = output_format(output_files[12], t_steps)
    df_14 = output_format(output_files[13], t_steps)
    df_15 = output_format(output_files[14], t_steps)

    frame = [df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10, df_11, df_12, df_13, df_14, df_15]

    df_output = pd.concat(frame)

    df_output.reset_index(drop=True, inplace=True)

    df_output.to_csv(output_displacement_filename, header=False, index=False)

    print(df_output)

    # ##################################################################################################################
    # ##################################################################################################################
    # ##################################################################################################################
    # ##################################################################################################################

    # FEATURE PREPROCESSING

    data_node_att = np.loadtxt(formatted_data_path + 'node_attributes.csv', delimiter=',')
    # --------------------------------------------------------------------------------------------------------------------
    # Node attribute format
    # {x} {y} {z} {material_ID} {Rigid} {Force magnitude} {F_x} {F_y} {F_z}
    #  0   1   2       3           4            5           6     7     8
    # material id 1: brain, 2: tumour
    # ----------------------------------------------------------------------------------------------------------------------

    # ############# Continuous encoding of rigid ID and material ID ###############
    physics_prop = np.zeros((data_node_att.shape[0], 1), dtype=float)

    for j in range(data_node_att.shape[0]):
        if data_node_att[j, 4] == 1:
            physics_prop[j, 0] = 0
        else:
            if data_node_att[j, 3] == 1:
                physics_prop[j, 0] = 1
            else:
                physics_prop[j, 0] = 0.4
    # ############################################################################
    # ############# Multiplication of force magnitude by direction ###############
    force_magnitude = data_node_att[:, 5]
    x_mag_and_direction = np.multiply(data_node_att[:, 6], force_magnitude)
    y_mag_and_direction = np.multiply(data_node_att[:, 7], force_magnitude)
    z_mag_and_direction = np.multiply(data_node_att[:, 8], force_magnitude)
    # ----------------------------------------------------------------------------------------------------------------------
    x_mag_and_direction = np.reshape(x_mag_and_direction, (-1, 1))
    y_mag_and_direction = np.reshape(y_mag_and_direction, (-1, 1))
    z_mag_and_direction = np.reshape(z_mag_and_direction, (-1, 1))

    # ----------------------------------------------------------------------------------------------------------------------
    feature_normalized = np.concatenate((data_node_att[:, 0:3], physics_prop, x_mag_and_direction, y_mag_and_direction,
                                         z_mag_and_direction), axis=1)
    # ----------------------------------------------------------------------------------------------------------------------
    print(feature_normalized)

    print(feature_normalized.shape)

    r = np.ptp(feature_normalized, axis=0)

    print(r)

    np.savetxt(formatted_data_path + "/node_attributes_raw.csv", feature_normalized, delimiter=",",
               fmt=('%f, %f, %f, %f, %f, %f, %f'))

# ----------------------------- Configuration 1 --------------------------------
# -----------------------------                  --------------------------------
class config1(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(config1, self).__init__()

        self.conv1 = tg_nn.GraphConv(input_dim, hidden_dim, aggr='max')
        self.conv2 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        self.conv3 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        self.conv4 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv5 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv6 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')

        self.jk1 = JumpingKnowledge("lstm", hidden_dim, 3)
        self.jk2 = JumpingKnowledge("lstm", hidden_dim, 3)

        self.lin1 = torch.nn.Linear(hidden_dim, 63)
        self.lin2 = torch.nn.Linear(63, 3)

        self.active1 = nn.PReLU(hidden_dim)
        self.active2 = nn.PReLU(hidden_dim)
        self.active3 = nn.PReLU(hidden_dim)
        self.active4 = nn.PReLU(hidden_dim)
        self.active5 = nn.PReLU(hidden_dim)
        self.active6 = nn.PReLU(hidden_dim)
        self.active7 = nn.PReLU(63)

    def forward(self, data):
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch
        edge_weight = 1 / edge_weight
        edge_weight = edge_weight.float()

        x = self.conv1(x, edge_index, edge_weight)
        x = self.active1(x)
        xs = [x]

        x = self.conv2(x, edge_index, edge_weight)
        x = self.active2(x)
        xs += [x]

        x = self.conv3(x, edge_index, edge_weight)
        x = self.active3(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk1(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.conv4(x, edge_index)
        x = self.active4(x)
        xs = [x]

        x = self.conv5(x, edge_index)
        x = self.active5(x)
        xs += [x]

        x = self.conv6(x, edge_index)
        x = self.active6(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk2(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.lin1(x)
        x = self.active7(x)
        x = F.dropout(x, p=0.1, training=self.training)
        x = self.lin2(x)

        return x

    def loss(self, pred, label):
        return (torch.sqrt(
            ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                        (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum()

        # return (pred - label).abs().sum()  #MAE
        # return F.mse_loss(pred, label)  #MSE

################################################################################

# ----------------------------- Configuration 2 --------------------------------
# -----------------------------                  --------------------------------
class config2(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(config2, self).__init__()

        self.conv1 = tg_nn.GraphConv(input_dim, hidden_dim, aggr='add')
        self.conv2 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        self.conv3 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        self.conv4 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv5 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv6 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')

        self.jk1 = JumpingKnowledge("lstm", hidden_dim, 3)
        self.jk2 = JumpingKnowledge("lstm", hidden_dim, 3)

        self.lin1 = torch.nn.Linear(hidden_dim, 63)
        self.lin2 = torch.nn.Linear(63, 3)

        self.active1 = nn.PReLU(hidden_dim)
        self.active2 = nn.PReLU(hidden_dim)
        self.active3 = nn.PReLU(hidden_dim)
        self.active4 = nn.PReLU(hidden_dim)
        self.active5 = nn.PReLU(hidden_dim)
        self.active6 = nn.PReLU(hidden_dim)
        self.active7 = nn.PReLU(63)

    def forward(self, data):
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch
        edge_weight = 1 / edge_weight
        edge_weight = edge_weight.float()

        x = self.conv1(x, edge_index, edge_weight)
        x = self.active1(x)
        xs = [x]

        x = self.conv2(x, edge_index, edge_weight)
        x = self.active2(x)
        xs += [x]

        x = self.conv3(x, edge_index, edge_weight)
        x = self.active3(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk1(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.conv4(x, edge_index)
        x = self.active4(x)
        xs = [x]

        x = self.conv5(x, edge_index)
        x = self.active5(x)
        xs += [x]

        x = self.conv6(x, edge_index)
        x = self.active6(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk2(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.lin1(x)
        x = self.active7(x)
        x = F.dropout(x, p=0.1, training=self.training)
        x = self.lin2(x)

        return x

    def loss(self, pred, label):
        return (torch.sqrt(
            ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                        (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum()

        # return (pred - label).abs().sum()  #MAE
        # return F.mse_loss(pred, label)  #MSE

################################################################################

# ----------------------------- Configuration 3 ---------------------------------
# -----------------------------                  --------------------------------
class config3(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(config3, self).__init__()

        self.conv1 = tg_nn.GraphConv(input_dim, hidden_dim, aggr='add')
        self.conv2 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='add')
        self.conv3 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        self.conv4 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv5 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv6 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')

        self.jk1 = JumpingKnowledge("lstm", hidden_dim, 3)
        self.jk2 = JumpingKnowledge("lstm", hidden_dim, 3)

        self.lin1 = torch.nn.Linear(hidden_dim, 63)
        self.lin2 = torch.nn.Linear(63, 3)

        self.active1 = nn.PReLU(hidden_dim)
        self.active2 = nn.PReLU(hidden_dim)
        self.active3 = nn.PReLU(hidden_dim)
        self.active4 = nn.PReLU(hidden_dim)
        self.active5 = nn.PReLU(hidden_dim)
        self.active6 = nn.PReLU(hidden_dim)
        self.active7 = nn.PReLU(63)

    def forward(self, data):
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch
        edge_weight = 1 / edge_weight
        edge_weight = edge_weight.float()

        x = self.conv1(x, edge_index, edge_weight)
        x = self.active1(x)
        xs = [x]

        x = self.conv2(x, edge_index, edge_weight)
        x = self.active2(x)
        xs += [x]

        x = self.conv3(x, edge_index, edge_weight)
        x = self.active3(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk1(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.conv4(x, edge_index)
        x = self.active4(x)
        xs = [x]

        x = self.conv5(x, edge_index)
        x = self.active5(x)
        xs += [x]

        x = self.conv6(x, edge_index)
        x = self.active6(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk2(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.lin1(x)
        x = self.active7(x)
        x = F.dropout(x, p=0.1, training=self.training)
        x = self.lin2(x)

        return x

    def loss(self, pred, label):
        return (torch.sqrt(
            ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                        (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum()

        # return (pred - label).abs().sum()  #MAE
        # return F.mse_loss(pred, label)  #MSE

################################################################################

# ----------------------------- Configuration 4 --------------------------------
# -----------------------------                  --------------------------------
class config4(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(config4, self).__init__()

        self.conv1 = tg_nn.GraphConv(input_dim, hidden_dim, aggr='add')
        self.conv2 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='add')
        self.conv3 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='add')
        self.conv4 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv5 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv6 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')

        self.jk1 = JumpingKnowledge("lstm", hidden_dim, 3)
        self.jk2 = JumpingKnowledge("lstm", hidden_dim, 3)

        self.lin1 = torch.nn.Linear(hidden_dim, 63)
        self.lin2 = torch.nn.Linear(63, 3)

        self.active1 = nn.PReLU(hidden_dim)
        self.active2 = nn.PReLU(hidden_dim)
        self.active3 = nn.PReLU(hidden_dim)
        self.active4 = nn.PReLU(hidden_dim)
        self.active5 = nn.PReLU(hidden_dim)
        self.active6 = nn.PReLU(hidden_dim)
        self.active7 = nn.PReLU(63)

    def forward(self, data):
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch
        edge_weight = 1 / edge_weight
        edge_weight = edge_weight.float()

        x = self.conv1(x, edge_index, edge_weight)
        x = self.active1(x)
        xs = [x]

        x = self.conv2(x, edge_index, edge_weight)
        x = self.active2(x)
        xs += [x]

        x = self.conv3(x, edge_index, edge_weight)
        x = self.active3(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk1(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.conv4(x, edge_index)
        x = self.active4(x)
        xs = [x]

        x = self.conv5(x, edge_index)
        x = self.active5(x)
        xs += [x]

        x = self.conv6(x, edge_index)
        x = self.active6(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk2(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.lin1(x)
        x = self.active7(x)
        x = F.dropout(x, p=0.1, training=self.training)
        x = self.lin2(x)

        return x

    def loss(self, pred, label):
        return (torch.sqrt(
            ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                        (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum()

        # return (pred - label).abs().sum()  #MAE
        # return F.mse_loss(pred, label)  #MSE

################################################################################

# ----------------------------- Configuration 5 --------------------------------
# -----------------------------                  --------------------------------
class config5(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(config5, self).__init__()

        self.conv1 = tg_nn.GraphConv(input_dim, hidden_dim, aggr='add')
        self.conv2 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='add')
        self.conv3 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='add')
        self.conv4 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='add')
        self.conv5 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv6 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')

        self.jk1 = JumpingKnowledge("lstm", hidden_dim, 3)
        self.jk2 = JumpingKnowledge("lstm", hidden_dim, 3)

        self.lin1 = torch.nn.Linear(hidden_dim, 63)
        self.lin2 = torch.nn.Linear(63, 3)

        self.active1 = nn.PReLU(hidden_dim)
        self.active2 = nn.PReLU(hidden_dim)
        self.active3 = nn.PReLU(hidden_dim)
        self.active4 = nn.PReLU(hidden_dim)
        self.active5 = nn.PReLU(hidden_dim)
        self.active6 = nn.PReLU(hidden_dim)
        self.active7 = nn.PReLU(63)

    def forward(self, data):
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch
        edge_weight = 1 / edge_weight
        edge_weight = edge_weight.float()

        x = self.conv1(x, edge_index, edge_weight)
        x = self.active1(x)
        xs = [x]

        x = self.conv2(x, edge_index, edge_weight)
        x = self.active2(x)
        xs += [x]

        x = self.conv3(x, edge_index, edge_weight)
        x = self.active3(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk1(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.conv4(x, edge_index)
        x = self.active4(x)
        xs = [x]

        x = self.conv5(x, edge_index)
        x = self.active5(x)
        xs += [x]

        x = self.conv6(x, edge_index)
        x = self.active6(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk2(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.lin1(x)
        x = self.active7(x)
        x = F.dropout(x, p=0.1, training=self.training)
        x = self.lin2(x)

        return x

    def loss(self, pred, label):
        return (torch.sqrt(
            ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                        (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum()

        # return (pred - label).abs().sum()  #MAE
        # return F.mse_loss(pred, label)  #MSE

################################################################################

# ----------------------------- Configuration 6 --------------------------------
# -----------------------------                  --------------------------------
class config6(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(config6, self).__init__()

        self.conv1 = tg_nn.GraphConv(input_dim, hidden_dim, aggr='add')
        self.conv2 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='add')
        self.conv3 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='add')
        self.conv4 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='add')
        self.conv5 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='add')
        self.conv6 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')

        self.jk1 = JumpingKnowledge("lstm", hidden_dim, 3)
        self.jk2 = JumpingKnowledge("lstm", hidden_dim, 3)

        self.lin1 = torch.nn.Linear(hidden_dim, 63)
        self.lin2 = torch.nn.Linear(63, 3)

        self.active1 = nn.PReLU(hidden_dim)
        self.active2 = nn.PReLU(hidden_dim)
        self.active3 = nn.PReLU(hidden_dim)
        self.active4 = nn.PReLU(hidden_dim)
        self.active5 = nn.PReLU(hidden_dim)
        self.active6 = nn.PReLU(hidden_dim)
        self.active7 = nn.PReLU(63)

    def forward(self, data):
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch
        edge_weight = 1 / edge_weight
        edge_weight = edge_weight.float()

        x = self.conv1(x, edge_index, edge_weight)
        x = self.active1(x)
        xs = [x]

        x = self.conv2(x, edge_index, edge_weight)
        x = self.active2(x)
        xs += [x]

        x = self.conv3(x, edge_index, edge_weight)
        x = self.active3(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk1(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.conv4(x, edge_index)
        x = self.active4(x)
        xs = [x]

        x = self.conv5(x, edge_index)
        x = self.active5(x)
        xs += [x]

        x = self.conv6(x, edge_index)
        x = self.active6(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk2(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.lin1(x)
        x = self.active7(x)
        x = F.dropout(x, p=0.1, training=self.training)
        x = self.lin2(x)

        return x

    def loss(self, pred, label):
        return (torch.sqrt(
            ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                        (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum()

        # return (pred - label).abs().sum()  #MAE
        # return F.mse_loss(pred, label)  #MSE

################################################################################

# ----------------------------- Configuration 7 --------------------------------
# -----------------------------                  --------------------------------
class config7(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(config7, self).__init__()

        self.conv1 = tg_nn.GraphConv(input_dim, hidden_dim, aggr='add')
        self.conv2 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='add')
        self.conv3 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='add')
        self.conv4 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='add')
        self.conv5 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='add')
        self.conv6 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='add')

        self.jk1 = JumpingKnowledge("lstm", hidden_dim, 3)
        self.jk2 = JumpingKnowledge("lstm", hidden_dim, 3)

        self.lin1 = torch.nn.Linear(hidden_dim, 63)
        self.lin2 = torch.nn.Linear(63, 3)

        self.active1 = nn.PReLU(hidden_dim)
        self.active2 = nn.PReLU(hidden_dim)
        self.active3 = nn.PReLU(hidden_dim)
        self.active4 = nn.PReLU(hidden_dim)
        self.active5 = nn.PReLU(hidden_dim)
        self.active6 = nn.PReLU(hidden_dim)
        self.active7 = nn.PReLU(63)

    def forward(self, data):
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch
        edge_weight = 1 / edge_weight
        edge_weight = edge_weight.float()

        x = self.conv1(x, edge_index, edge_weight)
        x = self.active1(x)
        xs = [x]

        x = self.conv2(x, edge_index, edge_weight)
        x = self.active2(x)
        xs += [x]

        x = self.conv3(x, edge_index, edge_weight)
        x = self.active3(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk1(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.conv4(x, edge_index)
        x = self.active4(x)
        xs = [x]

        x = self.conv5(x, edge_index)
        x = self.active5(x)
        xs += [x]

        x = self.conv6(x, edge_index)
        x = self.active6(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk2(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.lin1(x)
        x = self.active7(x)
        x = F.dropout(x, p=0.1, training=self.training)
        x = self.lin2(x)

        return x

    def loss(self, pred, label):
        return (torch.sqrt(
            ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                        (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum()

        # return (pred - label).abs().sum()  #MAE
        # return F.mse_loss(pred, label)  #MSE

################################################################################

# ----------------------------- Configuration 8 --------------------------------
# -----------------------------                  --------------------------------
class config8(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(config8, self).__init__()

        self.conv1 = tg_nn.GraphConv(input_dim, hidden_dim, aggr='add')

        if dataset_name == 'dataset_1':
            self.conv2 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        else:
            self.conv2 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='add')

        self.conv3 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv4 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv5 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv6 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')

        self.jk1 = JumpingKnowledge("lstm", hidden_dim, 3)
        self.jk2 = JumpingKnowledge("lstm", hidden_dim, 3)

        self.lin1 = torch.nn.Linear(hidden_dim, 63)
        self.lin2 = torch.nn.Linear(63, 3)

        self.active1 = nn.PReLU(hidden_dim)
        self.active2 = nn.PReLU(hidden_dim)
        self.active3 = nn.PReLU(hidden_dim)
        self.active4 = nn.PReLU(hidden_dim)
        self.active5 = nn.PReLU(hidden_dim)
        self.active6 = nn.PReLU(hidden_dim)
        self.active7 = nn.PReLU(63)

    def forward(self, data):
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch
        edge_weight = 1 / edge_weight
        edge_weight = edge_weight.float()

        x = self.conv1(x, edge_index, edge_weight)
        x = self.active1(x)
        xs = [x]

        x = self.conv2(x, edge_index)
        x = self.active2(x)
        xs += [x]

        x = self.conv3(x, edge_index)
        x = self.active3(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk1(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.conv4(x, edge_index)
        x = self.active4(x)
        xs = [x]

        x = self.conv5(x, edge_index)
        x = self.active5(x)
        xs += [x]

        x = self.conv6(x, edge_index)
        x = self.active6(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk2(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.lin1(x)
        x = self.active7(x)
        x = F.dropout(x, p=0.1, training=self.training)
        x = self.lin2(x)

        return x

    def loss(self, pred, label):

        return (torch.sqrt(
            ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                        (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum()

        # return (pred - label).abs().sum()  #MAE
        # return F.mse_loss(pred, label)  #MSE

################################################################################

# ----------------------------- Configuration 9 --------------------------------
# -----------------------------                  --------------------------------
class config9(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(config9, self).__init__()

        self.conv1 = tg_nn.GraphConv(input_dim, hidden_dim, aggr='add')

        if dataset_name == 'dataset_1':
            self.conv2 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        else:
            self.conv2 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='add')

        self.conv3 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv4 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv5 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv6 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')

        self.jk1 = JumpingKnowledge("lstm", hidden_dim, 3)
        self.jk2 = JumpingKnowledge("lstm", hidden_dim, 3)

        self.lin1 = torch.nn.Linear(hidden_dim, 63)
        self.lin2 = torch.nn.Linear(63, 3)

        self.active1 = nn.PReLU(hidden_dim)
        self.active2 = nn.PReLU(hidden_dim)
        self.active3 = nn.PReLU(hidden_dim)
        self.active4 = nn.PReLU(hidden_dim)
        self.active5 = nn.PReLU(hidden_dim)
        self.active6 = nn.PReLU(hidden_dim)
        self.active7 = nn.PReLU(63)

    def forward(self, data):
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch
        edge_weight = 1 / edge_weight
        edge_weight = edge_weight.float()

        x = self.conv1(x, edge_index, edge_weight)
        x = self.active1(x)
        xs = [x]

        x = self.conv2(x, edge_index, edge_weight)
        x = self.active2(x)
        xs += [x]

        x = self.conv3(x, edge_index)
        x = self.active3(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk1(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.conv4(x, edge_index)
        x = self.active4(x)
        xs = [x]

        x = self.conv5(x, edge_index)
        x = self.active5(x)
        xs += [x]

        x = self.conv6(x, edge_index)
        x = self.active6(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk2(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.lin1(x)
        x = self.active7(x)
        x = F.dropout(x, p=0.1, training=self.training)
        x = self.lin2(x)

        return x

    def loss(self, pred, label):

        return (torch.sqrt(
            ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                        (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum()

        # return (pred - label).abs().sum()  #MAE
        # return F.mse_loss(pred, label)  #MSE

################################################################################

# ----------------------------- Configuration 10 --------------------------------
# -----------------------------                  --------------------------------
class config10(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(config10, self).__init__()

        self.conv1 = tg_nn.GraphConv(input_dim, hidden_dim, aggr='add')

        if dataset_name == 'dataset_1':
            self.conv2 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        else:
            self.conv2 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='add')

        self.conv3 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        self.conv4 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        self.conv5 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')
        self.conv6 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')

        self.jk1 = JumpingKnowledge("lstm", hidden_dim, 3)
        self.jk2 = JumpingKnowledge("lstm", hidden_dim, 3)

        self.lin1 = torch.nn.Linear(hidden_dim, 63)
        self.lin2 = torch.nn.Linear(63, 3)

        self.active1 = nn.PReLU(hidden_dim)
        self.active2 = nn.PReLU(hidden_dim)
        self.active3 = nn.PReLU(hidden_dim)
        self.active4 = nn.PReLU(hidden_dim)
        self.active5 = nn.PReLU(hidden_dim)
        self.active6 = nn.PReLU(hidden_dim)
        self.active7 = nn.PReLU(63)

    def forward(self, data):
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch
        edge_weight = 1 / edge_weight
        edge_weight = edge_weight.float()

        x = self.conv1(x, edge_index, edge_weight)
        x = self.active1(x)
        xs = [x]

        x = self.conv2(x, edge_index, edge_weight)
        x = self.active2(x)
        xs += [x]

        x = self.conv3(x, edge_index, edge_weight)
        x = self.active3(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk1(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.conv4(x, edge_index, edge_weight)
        x = self.active4(x)
        xs = [x]

        x = self.conv5(x, edge_index)
        x = self.active5(x)
        xs += [x]

        x = self.conv6(x, edge_index)
        x = self.active6(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk2(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.lin1(x)
        x = self.active7(x)
        x = F.dropout(x, p=0.1, training=self.training)
        x = self.lin2(x)

        return x

    def loss(self, pred, label):

        return (torch.sqrt(
            ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                        (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum()

        # return (pred - label).abs().sum()  #MAE
        # return F.mse_loss(pred, label)  #MSE

################################################################################

# ----------------------------- Configuration 11 --------------------------------
# -----------------------------                  --------------------------------
class config11(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(config11, self).__init__()

        self.conv1 = tg_nn.GraphConv(input_dim, hidden_dim, aggr='add')

        if dataset_name == 'dataset_1':
            self.conv2 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        else:
            self.conv2 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='add')

        self.conv3 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        self.conv4 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        self.conv5 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        self.conv6 = tg_nn.SAGEConv(hidden_dim, hidden_dim, aggr='max')

        self.jk1 = JumpingKnowledge("lstm", hidden_dim, 3)
        self.jk2 = JumpingKnowledge("lstm", hidden_dim, 3)

        self.lin1 = torch.nn.Linear(hidden_dim, 63)
        self.lin2 = torch.nn.Linear(63, 3)

        self.active1 = nn.PReLU(hidden_dim)
        self.active2 = nn.PReLU(hidden_dim)
        self.active3 = nn.PReLU(hidden_dim)
        self.active4 = nn.PReLU(hidden_dim)
        self.active5 = nn.PReLU(hidden_dim)
        self.active6 = nn.PReLU(hidden_dim)
        self.active7 = nn.PReLU(63)

    def forward(self, data):
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch
        edge_weight = 1 / edge_weight
        edge_weight = edge_weight.float()

        x = self.conv1(x, edge_index, edge_weight)
        x = self.active1(x)
        xs = [x]

        x = self.conv2(x, edge_index, edge_weight)
        x = self.active2(x)
        xs += [x]

        x = self.conv3(x, edge_index, edge_weight)
        x = self.active3(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk1(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.conv4(x, edge_index, edge_weight)
        x = self.active4(x)
        xs = [x]

        x = self.conv5(x, edge_index, edge_weight)
        x = self.active5(x)
        xs += [x]

        x = self.conv6(x, edge_index)
        x = self.active6(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk2(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.lin1(x)
        x = self.active7(x)
        x = F.dropout(x, p=0.1, training=self.training)
        x = self.lin2(x)

        return x

    def loss(self, pred, label):

        return (torch.sqrt(
            ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                        (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum()

        # return (pred - label).abs().sum()  #MAE
        # return F.mse_loss(pred, label)  #MSE

################################################################################

# ----------------------------- Configuration 12 --------------------------------
# -----------------------------                  --------------------------------
class config12(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(config12, self).__init__()

        self.conv1 = tg_nn.GraphConv(input_dim, hidden_dim, aggr='add')

        if dataset_name == 'dataset_1':
            self.conv2 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        else:
            self.conv2 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='add')

        self.conv3 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        self.conv4 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        self.conv5 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')
        self.conv6 = tg_nn.GraphConv(hidden_dim, hidden_dim, aggr='max')

        self.jk1 = JumpingKnowledge("lstm", hidden_dim, 3)
        self.jk2 = JumpingKnowledge("lstm", hidden_dim, 3)

        self.lin1 = torch.nn.Linear(hidden_dim, 63)
        self.lin2 = torch.nn.Linear(63, 3)

        self.active1 = nn.PReLU(hidden_dim)
        self.active2 = nn.PReLU(hidden_dim)
        self.active3 = nn.PReLU(hidden_dim)
        self.active4 = nn.PReLU(hidden_dim)
        self.active5 = nn.PReLU(hidden_dim)
        self.active6 = nn.PReLU(hidden_dim)
        self.active7 = nn.PReLU(63)

    def forward(self, data):
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch
        edge_weight = 1 / edge_weight
        edge_weight = edge_weight.float()

        x = self.conv1(x, edge_index, edge_weight)
        x = self.active1(x)
        xs = [x]

        x = self.conv2(x, edge_index, edge_weight)
        x = self.active2(x)
        xs += [x]

        x = self.conv3(x, edge_index, edge_weight)
        x = self.active3(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk1(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.conv4(x, edge_index, edge_weight)
        x = self.active4(x)
        xs = [x]

        x = self.conv5(x, edge_index, edge_weight)
        x = self.active5(x)
        xs += [x]

        x = self.conv6(x, edge_index, edge_weight)
        x = self.active6(x)
        xs += [x]

        # ~~~~~~~~~~~~Jumping knowledge applied ~~~~~~~~~~~~~~~
        x = self.jk2(xs)
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        x = self.lin1(x)
        x = self.active7(x)
        x = F.dropout(x, p=0.1, training=self.training)
        x = self.lin2(x)

        return x

    def loss(self, pred, label):

        return (torch.sqrt(
            ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                        (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum()

        # return (pred - label).abs().sum()  #MAE
        # return F.mse_loss(pred, label)  #MSE

################################################################################
from library_imports import *
from models import *
from main import *


# Early Stopping
def early_stopping(val_loss, current_min):
    stop = False
    if val_loss[-1] > current_min:
        stop = True
    return stop


###############################################################################################

# Training
def train(dataset, writer, dataset_name, config_selected):
    data_size = len(dataset)
    print(dataset_name)
    # ------------------------------------------------------------------------------------------------------------------
    if dataset_name == 'dataset_1':
        print('Dataset 1 is being used.')
        loader = DataLoader(dataset[:int(data_size * 0.7)], batch_size=4, shuffle=True)
        validation_loader = DataLoader(dataset[int(data_size * 0.7): int(data_size * 0.9)], batch_size=1, shuffle=False)
        test_loader = DataLoader(dataset[int(data_size * 0.9):], batch_size=1, shuffle=False)
    else:
        print('Dataset 2 is being used.')
        loader = DataLoader(dataset[:int(data_size * 0.7)], batch_size=8, shuffle=True)
        validation_loader = DataLoader(dataset[int(data_size * 0.7): int(data_size * 0.9)], batch_size=1, shuffle=False)
        test_loader = DataLoader(dataset[int(data_size * 0.9):], batch_size=1, shuffle=False)
    # ------------------------------------------------------------------------------------------------------------------
    # MODIFICATION #1 Build the Model
    if config_selected == 'config1':
        model = config1(7, 112, 3)
        print('Configuration 1 is selected.')
    elif config_selected == 'config2':
        model = config2(7, 112, 3)
        print('Configuration 2 is selected.')
    elif config_selected == 'config3':
        model = config3(7, 112, 3)
        print('Configuration 3 is selected.')
    elif config_selected == 'config4':
        model = config4(7, 112, 3)
        print('Configuration 4 is selected.')
    elif config_selected == 'config5':
        model = config5(7, 112, 3)
        print('Configuration 5 is selected.')
    elif config_selected == 'config6':
        model = config6(7, 112, 3)
        print('Configuration 6 is selected.')
    elif config_selected == 'config7':
        model = config7(7, 112, 3)
        print('Configuration 7 is selected.')
    elif config_selected == 'config8':
        model = config8(7, 112, 3)
        print('Configuration 8 is selected.')
    elif config_selected == 'config9':
        model = config9(7, 112, 3)
        print('Configuration 9 is selected.')
    elif config_selected == 'config10':
        model = config10(7, 112, 3)
        print('Configuration 10 is selected.')
    elif config_selected == 'config11':
        model = config11(7, 112, 3)
        print('Configuration 11 is selected.')
    elif config_selected == 'config12':
        model = config12(7, 112, 3)
        print('Configuration 12 is selected.')
    else:
        raise NotImplementedError
    # ------------------------------------------------------------------------------------------------------------------
    # Move model to GPU
    model = model.to(device)
    # ------------------------------------------------------------------------------------------------------------------
    opt = optim.AdamW(model.parameters(), lr=0.005)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=5, min_lr=0.00000001,
                                                           verbose=True)
    # ------------------------------------------------------------------------------------------------------------------
    # print('parameters')
    # for name, param in model.named_parameters():
    #     if param.requires_grad:
    #         print (name, param.data)
    # print('----------------')
    # ------------------------------------------------------------------------------------------------------------------
    # for plotting
    learning_curve_train = []
    learning_curve_val = []
    # ------------------------------------------------------------------------------------------------------------------
    # for early stopping
    patience = 0
    # ------------------------------------------------------------------------------------------------------------------
    # MODIFICATION #2 Chossing the path to save the trained model
    if dataset_name == 'dataset_1':
        file_path = "Results_1/"
    else:
        file_path = "Results_2/"
    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    PATH = file_path + config_selected + '.pt'
    print(PATH)
    # ------------------------------------------------------------------------------------------------------------------
    learn_value_train = 0
    learn_value_validation = 0
    # train
    for epoch in range(15000):

        lr = scheduler.optimizer.param_groups[0]['lr']
        print(lr)

        total_loss = 0
        total_len = 0

        model.train()

        for batch in loader:
            batch = batch.to(device)

            opt.zero_grad()
            pred = model(batch)
            label = batch.y
            loss = model.loss(pred, label)
            loss = loss / len(batch.y)
            loss.backward()
            opt.step()
            # total_loss += loss.item()  # is loss isn't being averaged
            total_loss += loss.item() * len(batch.y)  # if loss is being averaged
            total_len += len(label)

        total_loss /= total_len  # train loss
        # ------------------------------------------------------------------------------------------------------------------
        # Validation
        val_loss = val(validation_loader, model)
        learning_curve_train.append(total_loss)
        learning_curve_val.append(val_loss)
        print("Epoch {}. Train Loss: {:.16f}. Validation Loss: {:.16f}".format(epoch, total_loss, val_loss))
        # ------------------------------------------------------------------------------------------------------------------
        scheduler.step(val_loss)
        # ------------------------------------------------------------------------------------------------------------------
        # Saving model and early stopping
        if epoch != 0:
            stop = early_stopping(learning_curve_val, current_min)
            if stop == False:
                current_min = learning_curve_val[-1]
                torch.save(model.state_dict(), PATH.format(epoch))
                patience = 0
            else:
                patience = patience + 1
                print(patience)
                if patience == 15:
                    break
        else:
            current_min = learning_curve_val[0]
    # ------------------------------------------------------------------------------
    # MODIFICATION #3
    if config_selected == 'config1':
        final_model = config1(7, 112, 3)
        print('Configuration 1 best state loading...')
    elif config_selected == 'config2':
        final_model = config2(7, 112, 3)
        print('Configuration 2 best state loading...')
    elif config_selected == 'config3':
        final_model = config3(7, 112, 3)
        print('Configuration 3 best state loading...')
    elif config_selected == 'config4':
        final_model = config4(7, 112, 3)
        print('Configuration 4 best state loading...')
    elif config_selected == 'config5':
        final_model = config5(7, 112, 3)
        print('Configuration 5 best state loading...')
    elif config_selected == 'config6':
        final_model = config6(7, 112, 3)
        print('Configuration 6 best state loading...')
    elif config_selected == 'config7':
        final_model = config7(7, 112, 3)
        print('Configuration 7 best state loading...')
    elif config_selected == 'config8':
        final_model = config8(7, 112, 3)
        print('Configuration 8 best state loading...')
    elif config_selected == 'config9':
        final_model = config9(7, 112, 3)
        print('Configuration 9 best state loading...')
    elif config_selected == 'config10':
        final_model = config10(7, 112, 3)
        print('Configuration 10 best state loading...')
    elif config_selected == 'config11':
        final_model = config11(7, 112, 3)
        print('Configuration 11 best state loading...')
    elif config_selected == 'config12':
        final_model = config12(7, 112, 3)
        print('Configuration 12 best state loading...')
    else:
        raise ValueError('The selected configuration does not exist.')
    # --------------------------------------------------------------------------
    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    final_model.load_state_dict(torch.load(PATH))
    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    # Final model to GPU
    final_model = final_model.to(device)
    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    lowest_val_loss = val(validation_loader, final_model)
    test_loss = test(test_loader, final_model)

    print("Validation Loss: {:.16f}. Test Loss: {:.16f}".format(lowest_val_loss, test_loss))
    print('---------------')
    # ------------------------------------------------------------------------------------------------------------------
    for j in range(len(learning_curve_train)):
        learn_value_train = learning_curve_train[j]
        learn_value_validation = learning_curve_val[j]
        print("{:.16f} {:.16f}".format(learn_value_train, learn_value_validation))

    return final_model


###############################################################################################
# Validation

def val(loader, model):
    model.eval()

    error = 0
    total_loss = 0
    total_length = 0

    with torch.no_grad():
        for batch in loader:
            batch = batch.to(device)

            pred = model(batch)
            label = batch.y
            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            # error = F.mse_loss(pred, label) # MSE
            # total_loss += error.item() * len(label) #MSE
            # ------------------------------------------------------------------
            # total_loss += (pred - label).abs().sum().item()  #MAE
            # ------------------------------------------------------------------
            total_loss += (torch.sqrt(
                ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                            (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum().item()  # MAE for magnitude
            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            total_length += len(label)

    total_loss /= total_length

    return total_loss


###############################################################################################
# Testing

def test(loader, model):
    model.eval()

    error = 0
    total_loss = 0
    total_length = 0

    with torch.no_grad():
        for data in loader:
            data = data.to(device)

            pred = model(data)
            label = data.y
            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            # error = F.mse_loss(pred, label) # MSE
            # total_loss += error.item() * len(label) #MSE
            # ------------------------------------------------------------------
            # total_loss += (pred - label).abs().sum().item()  #MAE
            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            total_loss += (torch.sqrt(
                ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                            (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum().item()  # MAE for magnitude
            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            total_length += len(label)

    total_loss /= total_length

    return total_loss


###############################################################################################


def val_reproduce(loader, model):
    model.eval()
    error = 0
    total_loss = 0
    total_length = 0

    actual = torch.empty(1, 1)
    prediction = torch.empty(1, 1)
    i = 0

    with torch.no_grad():
        for batch in loader:

            batch = batch.to(device)

            pred = model(batch)
            label = batch.y
            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            # to save prediction and actual labels
            if i == 0:
                prediction = pred
                actual = label
                i = i + 1
            else:
                prediction = torch.cat((prediction, pred), 0)
                actual = torch.cat((actual, label), 0)
                i = i + 1
            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            # error = F.mse_loss(pred, label) # MSE
            # total_loss += error.item() * len(label) #MSE
            # ------------------------------------------------------------------
            # total_loss += (pred - label).abs().sum().item()  #MAE
            # ------------------------------------------------------------------
            total_loss += (torch.sqrt(
                ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                            (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum().item()  # MAE for magnitude
            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            total_length += len(label)

    total_loss /= total_length

    return total_loss, prediction, actual


###############################################################################################


def test_reproduce(loader, model):
    model.eval()
    error = 0
    total_loss = 0
    total_length = 0

    actual = torch.empty(1, 1)
    prediction = torch.empty(1, 1)
    i = 0

    with torch.no_grad():
        for data in loader:
            data = data.to(device)
            pred = model(data)
            label = data.y
            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            # to save prediction and actual labels
            if i == 0:
                prediction = pred
                actual = label
                i = i + 1
            else:
                prediction = torch.cat((prediction, pred), 0)
                actual = torch.cat((actual, label), 0)
                i = i + 1
            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            # error = F.mse_loss(pred, label) # MSE
            # total_loss += error.item() * len(label) #MSE
            # ------------------------------------------------------------------
            # total_loss += (pred - label).abs().sum().item()  #MAE
            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            total_loss += (torch.sqrt(
                ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                            (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1))).sum().item()  # MAE for magnitude
            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            total_length += len(label)

    total_loss /= total_length

    return total_loss, prediction, actual


###############################################################################################


def max_magnitude_error(loader, model):
    model.eval()
    error = 0
    max_errors = []
    running_time = []
    max_displacement = []

    with torch.no_grad():
        for data in loader:
            data = data.to(device)

            t0 = time.time()
            pred = model(data)
            label = data.y
            t1 = time.time()
            total = t1 - t0

            running_time.append(total)

            loss = (torch.sqrt(
                ((pred[:, 0] - label[:, 0]) ** 2).unsqueeze(-1) + ((pred[:, 1] - label[:, 1]) ** 2).unsqueeze(-1) + (
                            (pred[:, 2] - label[:, 2]) ** 2).unsqueeze(-1)))  # MAE for magnitude

            max_loss = max(loss).item()
            print(max_loss)
            max_errors.append(max_loss)

            displacement = torch.sqrt(
                (label[:, 0] ** 2).unsqueeze(-1) + (label[:, 1] ** 2).unsqueeze(-1) + (label[:, 2] ** 2).unsqueeze(-1))
            max_displacement.append(max(displacement).item())

    return max_displacement, max_errors, running_time


def reproduce(dataset, writer, config_selected, save, mean_mag_results, max_error_results):
    data_size = len(dataset)
    # -----------------------------------------------------------------------------------------------
    if mean_mag_results == 1 and max_error_results == 0:
        validation_loader = DataLoader(dataset[int(data_size * 0.7): int(data_size * 0.9)], batch_size=8, shuffle=False)
        test_loader = DataLoader(dataset[int(data_size * 0.9):], batch_size=8, shuffle=False)
    else:
        validation_loader = DataLoader(dataset[int(data_size * 0.7): int(data_size * 0.9)], batch_size=1, shuffle=False)
        test_loader = DataLoader(dataset[int(data_size * 0.9):], batch_size=1, shuffle=False)
    # ------------------------------------------------------------------------------------------------------------------
    # Select the path to the file which holds the final model
    if dataset_name == 'dataset_1':
        file_path = "Results_1/"
    else:
        file_path = "Results_2/"
    # --------------------------------------------
    PATH = file_path + config_selected + '.pt'
    print(PATH)
    # --------------------------------------------
    # Initialize the model
    if config_selected == 'config1':
        final_model = config1(7, 112, 3)
        print('Configuration 1 has been selected.')
    elif config_selected == 'config2':
        final_model = config2(7, 112, 3)
        print('Configuration 2 has been selected.')
    elif config_selected == 'config3':
        final_model = config3(7, 112, 3)
        print('Configuration 3 has been selected.')
    elif config_selected == 'config4':
        final_model = config4(7, 112, 3)
        print('Configuration 4 has been selected.')
    elif config_selected == 'config5':
        final_model = config5(7, 112, 3)
        print('Configuration 5 has been selected.')
    elif config_selected == 'config6':
        final_model = config6(7, 112, 3)
        print('Configuration 6 has been selected.')
    elif config_selected == 'config7':
        final_model = config7(7, 112, 3)
        print('Configuration 7 has been selected.')
    elif config_selected == 'config8':
        final_model = config8(7, 112, 3)
        print('Configuration 8 has been selected.')
    elif config_selected == 'config9':
        final_model = config9(7, 112, 3)
        print('Configuration 9 has been selected.')
    elif config_selected == 'config10':
        final_model = config10(7, 112, 3)
        print('Configuration 10 has been selected.')
    elif config_selected == 'config11':
        final_model = config11(7, 112, 3)
        print('Configuration 11 has been selected.')
    elif config_selected == 'config12':
        final_model = config12(7, 112, 3)
        print('Configuration 12 has been selected.')
    else:
        raise NotImplementedError
        # ----------------------------------------------
    # Load the model
    final_model.load_state_dict(torch.load(PATH))
    # ----------------------------------------------
    # Final model to GPU
    final_model = final_model.to(device)
    # ----------------------------------------------
    # ######################################################################################################
    # Reproducing the mean of maximum errors in magnitude and runtime results
    if max_error_results == 1:
        max_displacement, max_error_test, running_time = max_magnitude_error(test_loader, final_model)

        max_maximum_displacement = max(max_displacement)
        print(max_maximum_displacement)
        mean_maximum_displacement = statistics.mean(max_displacement)
        print(mean_maximum_displacement)
        std_max_displacement = statistics.stdev(max_displacement)
        print(std_max_displacement)
        print("Maximum displacement in the test set: {:.4f}. Average maximum displacement {:.4f} +/- {:.4f}".format(
            max_maximum_displacement, mean_maximum_displacement, std_max_displacement))

        mean_maximum_test_loss = statistics.mean(max_error_test)
        std_maximum_test_loss = statistics.stdev(max_error_test)
        print(
            "Average maximum magnitude error: {:.4f} +/- {:.4f}".format(mean_maximum_test_loss, std_maximum_test_loss))

        mean_time = statistics.mean(running_time)
        std_time = statistics.stdev(running_time)
        print("Average running time: {:.4f} +/- {:.4f}".format(mean_time, std_time))

    # ######################################################################################################
    # Reproducing the results in Table 1 and Table 2
    if mean_mag_results == 1:

        if save == 0:
            lowest_val_loss = val(validation_loader, final_model)
            test_mse = test(test_loader, final_model)
            print("Validation Loss: {:.16f} Test Loss: {:.16f}".format(lowest_val_loss, test_mse))
        else:
            lowest_val_loss, prediction_val, actual_val = val_reproduce(validation_loader, final_model)
            test_mse, prediction_test, actual_test = test_reproduce(test_loader, final_model)
            print("Validation Loss: {:.16f} Test Loss: {:.16f}".format(lowest_val_loss, test_mse))

            # -----------------------------------------------------------------------------------------------
            # Saving the results for further analysis
            print('Saving the results...')

            # Validation set
            prediction_val = prediction_val.cpu()
            actual_val = actual_val.cpu()
            prediction_val = prediction_val.numpy()
            prediction_val_df = pd.DataFrame(prediction_val)
            # ------------------------------------------------------------------------------------------------
            prediction_val_df.to_csv(file_path + 'csv/val/prediction_' + config_selected + '.csv')
            # ------------------------------------------------------------------------------------------------
            actual_val = actual_val.numpy()
            actual_val_df = pd.DataFrame(actual_val)
            # ------------------------------------------------------------------------------------------------
            actual_val_df.to_csv(file_path + 'csv/val/actual_' + config_selected + '.csv')
            # ------------------------------------------------------------------------------------------------

            # Test set
            prediction_test = prediction_test.cpu()
            actual_test = actual_test.cpu()
            prediction_test = prediction_test.numpy()
            prediction_test_df = pd.DataFrame(prediction_test)
            # ------------------------------------------------------------------------------------------------
            prediction_test_df.to_csv(file_path + 'csv/test/prediction_' + config_selected + '.csv')
            # ------------------------------------------------------------------------------------------------
            actual_test = actual_test.numpy()
            actual_test_df = pd.DataFrame(actual_test)
            # ------------------------------------------------------------------------------------------------
            actual_test_df.to_csv(file_path + 'csv/test/actual_' + config_selected + '.csv')
            # ------------------------------------------------------------------------------------------------


